{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHD4X4djgmJX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import shuffle, class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Access grip names and file paths\n",
        "grip_files = {\n",
        "   'Tip': '/content/drive/MyDrive/Data/Tip.csv',\n",
        "   'Spherical': '/content/drive/MyDrive/Data/Spherical.csv',\n",
        "   'Palmer': '/content/drive/MyDrive/Data/Palmer.csv',\n",
        "   'Cylindrical': '/content/drive/MyDrive/Data/Cylindrical.csv',\n",
        "   'Hook': '/content/drive/MyDrive/Data/Hook.csv',\n",
        "   'Lateral': '/content/drive/MyDrive/Data/Lateral.csv',\n",
        "}\n",
        "\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features(signal, fs=500):\n",
        "   features = {}\n",
        "   features['MAV'] = np.mean(np.abs(signal))\n",
        "   features['RMS'] = np.sqrt(np.mean(signal**2))\n",
        "   features['VAR'] = np.var(signal)\n",
        "   features['IEMG'] = np.sum(np.abs(signal))\n",
        "   features['ZC'] = ((signal[:-1] * signal[1:]) < 0).sum()\n",
        "   features['WL'] = np.sum(np.abs(np.diff(signal)))\n",
        "   diff1 = np.diff(signal)\n",
        "   diff2 = np.diff(diff1)\n",
        "   features['SSC'] = np.sum((diff1[:-1] * diff1[1:] < 0) & (np.abs(diff2) > 0.01))\n",
        "   features['WAMP'] = np.sum(np.abs(np.diff(signal)) > 0.02)\n",
        "   features['LOG_D'] = np.exp(np.mean(np.log(np.abs(signal) + 1e-6)))\n",
        "\n",
        "\n",
        "   # Frequency-domain features\n",
        "   N = len(signal)\n",
        "   freqs = fftfreq(N, 1 / fs)\n",
        "   fft_vals = np.abs(fft(signal))\n",
        "   positive_freqs = freqs[:N // 2]\n",
        "   positive_fft = fft_vals[:N // 2]\n",
        "   total_power = np.sum(positive_fft)\n",
        "   features['Total_Power'] = total_power\n",
        "   features['MNF'] = np.sum(positive_freqs * positive_fft) / total_power\n",
        "   features['MDF'] = positive_freqs[np.where(np.cumsum(positive_fft) >= total_power / 2)[0][0]]\n",
        "\n",
        "\n",
        "   return features\n",
        "\n",
        "\n",
        "# Sliding window parameters\n",
        "window_size = 50\n",
        "step_size = 10\n",
        "\n",
        "\n",
        "# Build feature dataset\n",
        "feature_rows = []\n",
        "\n",
        "\n",
        "for grip_label, file_path in grip_files.items():\n",
        "   df = pd.read_csv(file_path, header=None)\n",
        "   ch1 = df.iloc[:, 0].values\n",
        "   ch2 = df.iloc[:, 1].values\n",
        "\n",
        "\n",
        "   for i in range(0, len(ch1) - window_size + 1, step_size):\n",
        "       window1 = ch1[i:i + window_size]\n",
        "       window2 = ch2[i:i + window_size]\n",
        "\n",
        "\n",
        "       feat1 = extract_features(window1)\n",
        "       feat2 = extract_features(window2)\n",
        "\n",
        "\n",
        "       combined = {f'EMG1_{k}': v for k, v in feat1.items()}\n",
        "       combined.update({f'EMG2_{k}': v for k, v in feat2.items()})\n",
        "       combined['Grip'] = grip_label\n",
        "       feature_rows.append(combined)\n",
        "\n",
        "\n",
        "# Convert to dataframe\n",
        "features_df = pd.DataFrame(feature_rows)\n",
        "\n",
        "\n",
        "# grip labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_labels = label_encoder.fit_transform(features_df['Grip'])\n",
        "y = to_categorical(y_labels, num_classes=6)\n",
        "\n",
        "\n",
        "# Prepare features\n",
        "X = features_df.drop(columns='Grip').values\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "   Dense(200, activation='relu', input_dim=X_train.shape[1]),\n",
        "   BatchNormalization(),\n",
        "   Dropout(0.2),\n",
        "\n",
        "\n",
        "   Dense(80, activation='relu'),\n",
        "   BatchNormalization(),\n",
        "   Dropout(0.2),\n",
        "\n",
        "\n",
        "   Dense(64, activation='relu'),\n",
        "   BatchNormalization(),\n",
        "   Dropout(0.2),\n",
        "\n",
        "\n",
        "   Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-4)\n",
        "\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "   X_train, y_train,\n",
        "   validation_data=(X_test, y_test),\n",
        "   epochs=15,\n",
        "   batch_size=50,\n",
        "   class_weight=class_weights_dict,\n",
        "   callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "\n",
        "# Predict labels\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"✅ Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert to TFLite full integer quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "\n",
        "def representative_dataset():\n",
        "   for i in range(min(100, len(X_train))):\n",
        "       yield [X_train[i:i+1].astype(np.float32)]\n",
        "\n",
        "\n",
        "converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"grip_model_quant.tflite\", \"wb\") as f:\n",
        "   f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"grip_model_quant.tflite\")\n",
        "\n",
        "\n",
        "# Load and evaluate quantized model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"grip_model_quant.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "# Get quantization parameters\n",
        "input_scale = input_details[0]['quantization'][0]\n",
        "input_zero_point = input_details[0]['quantization'][1]\n",
        "\n",
        "\n",
        "# Evaluate on test data\n",
        "correct = 0\n",
        "total = len(X_test)\n",
        "\n",
        "\n",
        "print(\"\\nQuantization parameters:\")\n",
        "print(f\"Input scale: {input_scale}\")\n",
        "print(f\"Input zero point: {input_zero_point}\")\n",
        "\n",
        "\n",
        "for i in range(total):\n",
        "   # Quantize the input data\n",
        "   input_data = X_test[i].astype(np.float32)\n",
        "   input_data_quantized = np.array(input_data / input_scale + input_zero_point, dtype=np.int8)\n",
        "   input_data_quantized = np.expand_dims(input_data_quantized, axis=0)\n",
        "\n",
        "\n",
        "   interpreter.set_tensor(input_details[0]['index'], input_data_quantized)\n",
        "   interpreter.invoke()\n",
        "   output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "   # Dequantize the output\n",
        "   output_scale = output_details[0]['quantization'][0]\n",
        "   output_zero_point = output_details[0]['quantization'][1]\n",
        "   output_data_dequantized = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
        "\n",
        "\n",
        "   if np.argmax(output_data_dequantized) == np.argmax(y_test[i]):\n",
        "       correct += 1\n",
        "\n",
        "\n",
        "quant_acc = correct / total\n",
        "print(f\"⚡ Quantized Model Accuracy: {quant_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Print quantization parameters for C++ code\n",
        "print(\"\\nFor C++ code:\")\n",
        "print(f\"Input scale: {input_scale}\")\n",
        "print(f\"Input zero point: {input_zero_point}\")\n",
        "print(f\"Output scale: {output_scale}\")\n",
        "print(f\"Output zero point: {output_zero_point}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "grip_names = label_encoder.classes_\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=grip_names, yticklabels=grip_names)\n",
        "plt.xlabel('Predicted Grip')\n",
        "plt.ylabel('True Grip')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}